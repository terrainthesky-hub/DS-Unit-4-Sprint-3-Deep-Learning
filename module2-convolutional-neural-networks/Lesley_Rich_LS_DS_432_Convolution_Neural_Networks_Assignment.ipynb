{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "U4-S2-NNF-DS10",
      "language": "python",
      "name": "u4-s2-nnf-ds10"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "nteract": {
      "version": "0.23.1"
    },
    "colab": {
      "name": "Lesley_Rich_LS_DS_432_Convolution_Neural_Networks_Assignment.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/terrainthesky-hub/DS-Unit-4-Sprint-3-Deep-Learning/blob/main/module2-convolutional-neural-networks/Lesley_Rich_LS_DS_432_Convolution_Neural_Networks_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7VjmuLh6xRe",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 3 Assignment 2*\n",
        "# Convolutional Neural Networks (CNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0lfZdD_cp1t5"
      },
      "source": [
        "# Assignment\n",
        "\n",
        "- <a href=\"#p1\">Part 1:</a> Pre-Trained Model\n",
        "- <a href=\"#p2\">Part 2:</a> Custom CNN Model\n",
        "- <a href=\"#p3\">Part 3:</a> CNN with Data Augmentation\n",
        "\n",
        "\n",
        "You will apply three different CNN models to a binary image classification model using Keras. Classify images of Mountains (`./data/train/mountain/*`) and images of forests (`./data/train/forest/*`). Treat mountains as the positive class (1) and the forest images as the negative (zero). \n",
        "\n",
        "|Mountain (+)|Forest (-)|\n",
        "|---|---|\n",
        "|![](./data/train/mountain/art1131.jpg)|![](./data/validation/forest/cdmc317.jpg)|\n",
        "\n",
        "The problem is relatively difficult given that the sample is tiny: there are about 350 observations per class. This sample size might be something that you can expect with prototyping an image classification problem/solution at work. Get accustomed to evaluating several different possible models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VJhEx7cZ6xRf"
      },
      "source": [
        "# Pre - Trained Model\n",
        "<a id=\"p1\"></a>\n",
        "\n",
        "Load a pretrained network from Keras, [ResNet50](https://tfhub.dev/google/imagenet/resnet_v1_50/classification/1) - a 50 layer deep network trained to recognize [1000 objects](https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt). Starting usage:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model # This is the functional API\n",
        "\n",
        "resnet = ResNet50(weights='imagenet', include_top=False)\n",
        "\n",
        "```\n",
        "\n",
        "The `include_top` parameter in `ResNet50` will remove the full connected layers from the ResNet model. The next step is to turn off the training of the ResNet layers. We want to use the learned parameters without updating them in future training passes. \n",
        "\n",
        "```python\n",
        "for layer in resnet.layers:\n",
        "    layer.trainable = False\n",
        "```\n",
        "\n",
        "Using the Keras functional API, we will need to additional additional full connected layers to our model. We we removed the top layers, we removed all preivous fully connected layers. In other words, we kept only the feature processing portions of our network. You can expert with additional layers beyond what's listed here. The `GlobalAveragePooling2D` layer functions as a really fancy flatten function by taking the average of each of the last convolutional layer outputs (which is two dimensional still). \n",
        "\n",
        "```python\n",
        "x = resnet.output\n",
        "x = GlobalAveragePooling2D()(x) # This layer is a really fancy flatten\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(resnet.input, predictions)\n",
        "```\n",
        "\n",
        "Your assignment is to apply the transfer learning above to classify images of Mountains (`./data/train/mountain/*`) and images of forests (`./data/train/forest/*`). Treat mountains as the positive class (1) and the forest images as the negative (zero). \n",
        "\n",
        "Steps to complete assignment: \n",
        "1. Load in Image Data into numpy arrays (`X`) \n",
        "2. Create a `y` for the labels\n",
        "3. Train your model with pre-trained layers from resnet\n",
        "4. Report your model's accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlFVc9U_6xRg",
        "colab_type": "text"
      },
      "source": [
        "## Load in Data\n",
        "\n",
        "This surprisingly more difficult than it seems, because you are working with directories of images instead of a single file. This boiler plate will help you download a zipped version of the directory of images. The directory is organized into \"train\" and \"validation\" which you can use inside an `ImageGenerator` class to stream batches of images thru your model.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSQPPO4Y6xRg",
        "colab_type": "text"
      },
      "source": [
        "### Download & Summarize the Data\n",
        "\n",
        "This step is completed for you. Just run the cells and review the results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W07r6h9f6xRh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cf55c289-e379-4125-b4ad-53f107433539"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "_URL = 'https://lambdaschool-data-science.s3.amazonaws.com/mountains_v_forest/mountain_v_forest_data.zip'\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file('./data.zip', origin=_URL, extract=True)\n",
        "PATH = os.path.join(os.path.dirname(path_to_zip), 'data')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://lambdaschool-data-science.s3.amazonaws.com/mountains_v_forest/mountain_v_forest_data.zip\n",
            "375046144/375045706 [==============================] - 4s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0tZc0Vy6xRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dir = os.path.join(PATH, 'train')\n",
        "validation_dir = os.path.join(PATH, 'validation')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jhu_yMmd6xRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_mountain_dir = os.path.join(train_dir, 'mountain')  # directory with our training cat pictures\n",
        "train_forest_dir = os.path.join(train_dir, 'forest')  # directory with our training dog pictures\n",
        "validation_mountain_dir = os.path.join(validation_dir, 'mountain')  # directory with our validation cat pictures\n",
        "validation_forest_dir = os.path.join(validation_dir, 'forest')  # directory with our validation dog pictures"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6Mc6bp86xRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_mountain_tr = len(os.listdir(train_mountain_dir))\n",
        "num_forest_tr = len(os.listdir(train_forest_dir))\n",
        "\n",
        "num_mountain_val = len(os.listdir(validation_mountain_dir))\n",
        "num_forest_val = len(os.listdir(validation_forest_dir))\n",
        "\n",
        "total_train = num_mountain_tr + num_forest_tr\n",
        "total_val = num_mountain_val + num_forest_val"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzlX9XCA6xRt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "3401739c-fabb-4d79-b716-5581092fd69d"
      },
      "source": [
        "print('total training mountain images:', num_mountain_tr)\n",
        "print('total training forest images:', num_forest_tr)\n",
        "\n",
        "print('total validation mountain images:', num_mountain_val)\n",
        "print('total validation forest images:', num_forest_val)\n",
        "print(\"--\")\n",
        "print(\"Total training images:\", total_train)\n",
        "print(\"Total validation images:\", total_val)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total training mountain images: 296\n",
            "total training forest images: 326\n",
            "total validation mountain images: 125\n",
            "total validation forest images: 62\n",
            "--\n",
            "Total training images: 622\n",
            "Total validation images: 187\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s11Sez5S6xRv",
        "colab_type": "text"
      },
      "source": [
        "### Keras `ImageGenerator` to Process the Data\n",
        "\n",
        "This step is completed for you, but please review the code. The `ImageGenerator` class reads in batches of data from a directory and pass them to the model one batch at a time. Just like large text files, this method is advantageous, because it stifles the need to load a bunch of images into memory. \n",
        "\n",
        "Check out the documentation for this class method: [Keras `ImageGenerator` Class](https://keras.io/preprocessing/image/#imagedatagenerator-class). You'll expand it's use in the third assignment objective."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwxqPgKD6xRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 16\n",
        "epochs = 50\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1XgNpZ96xRy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our training data\n",
        "validation_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our validation data"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ST9GR4dI6xR1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "321e440b-7074-413f-eda7-a3ab5f6ae62e"
      },
      "source": [
        "train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                           directory=train_dir,\n",
        "                                                           shuffle=True,\n",
        "                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                           class_mode='binary')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 631 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzABW6Z36xR3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0e5fc2c0-2423-4d26-e594-b00d7d54c724"
      },
      "source": [
        "val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                              directory=validation_dir,\n",
        "                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                              class_mode='binary')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 195 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8FbwCfz6xR5",
        "colab_type": "text"
      },
      "source": [
        "## Instatiate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzLDg8O06xR5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import numpy as np\n",
        "\n",
        "# from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "# from tensorflow.keras.preprocessing import image\n",
        "# from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "\n",
        "# def process_img_path(img_path):\n",
        "#   return image.load_img(img_path, target_size=(224, 224))\n",
        "\n",
        "# def img_contains_banana(img):\n",
        "#   x = image.img_to_array(img)\n",
        "#   x = np.expand_dims(x, axis=0)\n",
        "#   x = preprocess_input(x)\n",
        "#   model = ResNet50(weights='imagenet')\n",
        "#   features = model.predict(x)\n",
        "#   results = decode_predictions(features, top=3)[0]\n",
        "#   print(results)\n",
        "#   for entry in results:\n",
        "#     if entry[1] == 'mountain':\n",
        "#       return entry[2]\n",
        "#   return 0.0"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Yi-ziUGBZxw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c0b5e7c3-b5b5-406f-e964-256b84d61d57"
      },
      "source": [
        "import numpy as np\n",
        " \n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        " \n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model # This is the functional API\n",
        "\n",
        "model = ResNet50(weights='imagenet', include_top=False)\n",
        "for layer in model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "x = model.output\n",
        "x = GlobalAveragePooling2D()(x) # This layer is a really fancy flatten\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(model.input, predictions)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nqrbx8x0B9Fv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cprqp7R16xR7",
        "colab_type": "text"
      },
      "source": [
        "## Fit Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND2K48pM6xR8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "958d55b7-667f-4e41-8998-db7687a56477"
      },
      "source": [
        "history = model.fit(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch=total_train // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_data_gen,\n",
        "    validation_steps=total_val // batch_size\n",
        ")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "38/38 [==============================] - 35s 921ms/step - loss: 0.8900 - accuracy: 0.5259 - val_loss: 0.8248 - val_accuracy: 0.3295\n",
            "Epoch 2/50\n",
            "38/38 [==============================] - 38s 1s/step - loss: 0.6168 - accuracy: 0.6427 - val_loss: 0.5390 - val_accuracy: 0.8295\n",
            "Epoch 3/50\n",
            "38/38 [==============================] - 37s 968ms/step - loss: 0.5074 - accuracy: 0.7980 - val_loss: 0.5551 - val_accuracy: 0.7670\n",
            "Epoch 4/50\n",
            "38/38 [==============================] - 37s 969ms/step - loss: 0.4514 - accuracy: 0.8114 - val_loss: 0.4213 - val_accuracy: 0.8864\n",
            "Epoch 5/50\n",
            "38/38 [==============================] - 37s 983ms/step - loss: 0.4215 - accuracy: 0.8147 - val_loss: 0.4907 - val_accuracy: 0.8011\n",
            "Epoch 6/50\n",
            "38/38 [==============================] - 37s 961ms/step - loss: 0.3741 - accuracy: 0.8548 - val_loss: 0.3813 - val_accuracy: 0.8693\n",
            "Epoch 7/50\n",
            "38/38 [==============================] - 36s 936ms/step - loss: 0.4327 - accuracy: 0.7813 - val_loss: 0.9730 - val_accuracy: 0.4716\n",
            "Epoch 8/50\n",
            "38/38 [==============================] - 38s 993ms/step - loss: 0.4155 - accuracy: 0.7997 - val_loss: 0.3539 - val_accuracy: 0.8750\n",
            "Epoch 9/50\n",
            "38/38 [==============================] - 35s 925ms/step - loss: 0.3425 - accuracy: 0.8381 - val_loss: 0.2993 - val_accuracy: 0.9261\n",
            "Epoch 10/50\n",
            "38/38 [==============================] - 36s 949ms/step - loss: 0.3236 - accuracy: 0.8750 - val_loss: 0.3107 - val_accuracy: 0.8864\n",
            "Epoch 11/50\n",
            "38/38 [==============================] - 36s 943ms/step - loss: 0.2901 - accuracy: 0.8881 - val_loss: 0.3081 - val_accuracy: 0.8807\n",
            "Epoch 12/50\n",
            "38/38 [==============================] - 36s 938ms/step - loss: 0.2725 - accuracy: 0.9215 - val_loss: 0.2836 - val_accuracy: 0.8864\n",
            "Epoch 13/50\n",
            "38/38 [==============================] - 36s 946ms/step - loss: 0.2964 - accuracy: 0.8798 - val_loss: 0.2467 - val_accuracy: 0.9375\n",
            "Epoch 14/50\n",
            "38/38 [==============================] - 38s 988ms/step - loss: 0.2317 - accuracy: 0.9265 - val_loss: 0.2290 - val_accuracy: 0.9375\n",
            "Epoch 15/50\n",
            "38/38 [==============================] - 34s 903ms/step - loss: 0.2720 - accuracy: 0.8865 - val_loss: 0.6104 - val_accuracy: 0.6989\n",
            "Epoch 16/50\n",
            "38/38 [==============================] - 37s 986ms/step - loss: 0.2598 - accuracy: 0.9048 - val_loss: 0.3004 - val_accuracy: 0.8239\n",
            "Epoch 17/50\n",
            "38/38 [==============================] - 36s 939ms/step - loss: 0.2463 - accuracy: 0.9065 - val_loss: 0.4067 - val_accuracy: 0.8182\n",
            "Epoch 18/50\n",
            "38/38 [==============================] - 37s 962ms/step - loss: 0.2406 - accuracy: 0.9082 - val_loss: 0.2719 - val_accuracy: 0.8920\n",
            "Epoch 19/50\n",
            "38/38 [==============================] - 38s 1s/step - loss: 0.2094 - accuracy: 0.9249 - val_loss: 0.5204 - val_accuracy: 0.7614\n",
            "Epoch 20/50\n",
            "38/38 [==============================] - 36s 941ms/step - loss: 0.2632 - accuracy: 0.8798 - val_loss: 0.2728 - val_accuracy: 0.8750\n",
            "Epoch 21/50\n",
            "38/38 [==============================] - 36s 942ms/step - loss: 0.1925 - accuracy: 0.9332 - val_loss: 0.7634 - val_accuracy: 0.6477\n",
            "Epoch 22/50\n",
            "38/38 [==============================] - 38s 996ms/step - loss: 0.2180 - accuracy: 0.9065 - val_loss: 0.3144 - val_accuracy: 0.8636\n",
            "Epoch 23/50\n",
            "38/38 [==============================] - 37s 964ms/step - loss: 0.4008 - accuracy: 0.8214 - val_loss: 0.5170 - val_accuracy: 0.7670\n",
            "Epoch 24/50\n",
            "38/38 [==============================] - 36s 960ms/step - loss: 0.1988 - accuracy: 0.9215 - val_loss: 0.5618 - val_accuracy: 0.7500\n",
            "Epoch 25/50\n",
            "38/38 [==============================] - 35s 931ms/step - loss: 0.2308 - accuracy: 0.9115 - val_loss: 0.3897 - val_accuracy: 0.8352\n",
            "Epoch 26/50\n",
            "38/38 [==============================] - 37s 986ms/step - loss: 0.1971 - accuracy: 0.9282 - val_loss: 0.1931 - val_accuracy: 0.9432\n",
            "Epoch 27/50\n",
            "38/38 [==============================] - 36s 958ms/step - loss: 0.1869 - accuracy: 0.9265 - val_loss: 0.2797 - val_accuracy: 0.8864\n",
            "Epoch 28/50\n",
            "38/38 [==============================] - 37s 969ms/step - loss: 0.2062 - accuracy: 0.9115 - val_loss: 0.2411 - val_accuracy: 0.9205\n",
            "Epoch 29/50\n",
            "38/38 [==============================] - 35s 930ms/step - loss: 0.2405 - accuracy: 0.9048 - val_loss: 0.2326 - val_accuracy: 0.9261\n",
            "Epoch 30/50\n",
            "38/38 [==============================] - 34s 901ms/step - loss: 0.1499 - accuracy: 0.9449 - val_loss: 0.1685 - val_accuracy: 0.9432\n",
            "Epoch 31/50\n",
            "38/38 [==============================] - 36s 945ms/step - loss: 0.1554 - accuracy: 0.9482 - val_loss: 0.2387 - val_accuracy: 0.9034\n",
            "Epoch 32/50\n",
            "38/38 [==============================] - 37s 980ms/step - loss: 0.2609 - accuracy: 0.8982 - val_loss: 0.2449 - val_accuracy: 0.9091\n",
            "Epoch 33/50\n",
            "38/38 [==============================] - 38s 1s/step - loss: 0.4637 - accuracy: 0.8130 - val_loss: 0.2371 - val_accuracy: 0.9261\n",
            "Epoch 34/50\n",
            "38/38 [==============================] - 37s 967ms/step - loss: 0.1829 - accuracy: 0.9249 - val_loss: 0.2642 - val_accuracy: 0.8864\n",
            "Epoch 35/50\n",
            "38/38 [==============================] - 34s 903ms/step - loss: 0.1849 - accuracy: 0.9349 - val_loss: 0.1898 - val_accuracy: 0.9489\n",
            "Epoch 36/50\n",
            "38/38 [==============================] - 37s 970ms/step - loss: 0.1905 - accuracy: 0.9227 - val_loss: 0.3925 - val_accuracy: 0.8011\n",
            "Epoch 37/50\n",
            "38/38 [==============================] - 37s 980ms/step - loss: 0.1610 - accuracy: 0.9499 - val_loss: 0.2494 - val_accuracy: 0.8977\n",
            "Epoch 38/50\n",
            "38/38 [==============================] - 37s 965ms/step - loss: 0.1653 - accuracy: 0.9449 - val_loss: 0.3799 - val_accuracy: 0.8295\n",
            "Epoch 39/50\n",
            "38/38 [==============================] - 38s 992ms/step - loss: 0.2037 - accuracy: 0.9199 - val_loss: 0.2195 - val_accuracy: 0.9318\n",
            "Epoch 40/50\n",
            "38/38 [==============================] - 36s 956ms/step - loss: 0.1498 - accuracy: 0.9449 - val_loss: 0.1699 - val_accuracy: 0.9545\n",
            "Epoch 41/50\n",
            "38/38 [==============================] - 37s 970ms/step - loss: 0.2254 - accuracy: 0.9048 - val_loss: 0.7428 - val_accuracy: 0.6705\n",
            "Epoch 42/50\n",
            "38/38 [==============================] - 36s 956ms/step - loss: 0.1999 - accuracy: 0.9265 - val_loss: 0.2603 - val_accuracy: 0.9034\n",
            "Epoch 43/50\n",
            "38/38 [==============================] - 37s 963ms/step - loss: 0.1878 - accuracy: 0.9232 - val_loss: 0.4242 - val_accuracy: 0.8011\n",
            "Epoch 44/50\n",
            "38/38 [==============================] - 38s 1s/step - loss: 0.1678 - accuracy: 0.9366 - val_loss: 0.2753 - val_accuracy: 0.8977\n",
            "Epoch 45/50\n",
            "38/38 [==============================] - 37s 961ms/step - loss: 0.1386 - accuracy: 0.9566 - val_loss: 0.2323 - val_accuracy: 0.9261\n",
            "Epoch 46/50\n",
            "38/38 [==============================] - 38s 989ms/step - loss: 0.1762 - accuracy: 0.9349 - val_loss: 0.1806 - val_accuracy: 0.9545\n",
            "Epoch 47/50\n",
            "38/38 [==============================] - 37s 965ms/step - loss: 0.1328 - accuracy: 0.9482 - val_loss: 0.1713 - val_accuracy: 0.9489\n",
            "Epoch 48/50\n",
            "38/38 [==============================] - 37s 971ms/step - loss: 0.1266 - accuracy: 0.9482 - val_loss: 0.2816 - val_accuracy: 0.8977\n",
            "Epoch 49/50\n",
            "38/38 [==============================] - 36s 952ms/step - loss: 0.1330 - accuracy: 0.9466 - val_loss: 0.1778 - val_accuracy: 0.9432\n",
            "Epoch 50/50\n",
            "38/38 [==============================] - 36s 955ms/step - loss: 0.1364 - accuracy: 0.9499 - val_loss: 0.3832 - val_accuracy: 0.8409\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_uawi9r6xR9",
        "colab_type": "text"
      },
      "source": [
        "# Custom CNN Model\n",
        "\n",
        "In this step, write and train your own convolutional neural network using Keras. You can use any architecture that suits you as long as it has at least one convolutional and one pooling layer at the beginning of the network - you can add more if you want. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esrpm73w6xR-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "fc0e6ce8-0e21-4ae9-a732-03a7d594f321"
      },
      "source": [
        "# Define the Model\n",
        "from tensorflow.keras import datasets\n",
        "from tensorflow.keras.models import Sequential, Model # <- May Use\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), activation='relu', input_shape=(224, 224, 3)))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "model.add(Flatten()) #flatten before going to fully connected layer (before final)\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 222, 222, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 111, 111, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 109, 109, 64)      18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 54, 54, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 52, 52, 64)        36928     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 173056)            0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                11075648  \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 11,132,033\n",
            "Trainable params: 11,132,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUH2iDNhLHeQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS17eTLi6xR_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHJDlFvH6xSD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fdad503d-a529-4232-b7a0-426d8da8edf5"
      },
      "source": [
        "# Fit Model\n",
        "history2 = model.fit(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch=total_train // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_data_gen,\n",
        "    validation_steps=total_val // batch_size\n",
        ")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "38/38 [==============================] - 34s 897ms/step - loss: 0.7135 - accuracy: 0.7329 - val_loss: 0.2647 - val_accuracy: 0.9148\n",
            "Epoch 2/50\n",
            "38/38 [==============================] - 35s 926ms/step - loss: 0.2901 - accuracy: 0.8798 - val_loss: 0.2143 - val_accuracy: 0.9205\n",
            "Epoch 3/50\n",
            "38/38 [==============================] - 37s 976ms/step - loss: 0.2146 - accuracy: 0.9215 - val_loss: 0.1838 - val_accuracy: 0.9261\n",
            "Epoch 4/50\n",
            "38/38 [==============================] - 36s 946ms/step - loss: 0.2389 - accuracy: 0.9082 - val_loss: 0.1671 - val_accuracy: 0.9205\n",
            "Epoch 5/50\n",
            "38/38 [==============================] - 36s 935ms/step - loss: 0.1977 - accuracy: 0.9332 - val_loss: 0.2985 - val_accuracy: 0.8636\n",
            "Epoch 6/50\n",
            "38/38 [==============================] - 35s 928ms/step - loss: 0.2340 - accuracy: 0.8998 - val_loss: 0.1941 - val_accuracy: 0.9205\n",
            "Epoch 7/50\n",
            "38/38 [==============================] - 36s 950ms/step - loss: 0.1978 - accuracy: 0.9199 - val_loss: 0.1471 - val_accuracy: 0.9489\n",
            "Epoch 8/50\n",
            "38/38 [==============================] - 37s 967ms/step - loss: 0.1378 - accuracy: 0.9516 - val_loss: 0.1690 - val_accuracy: 0.9205\n",
            "Epoch 9/50\n",
            "38/38 [==============================] - 35s 925ms/step - loss: 0.1364 - accuracy: 0.9416 - val_loss: 0.2612 - val_accuracy: 0.8864\n",
            "Epoch 10/50\n",
            "38/38 [==============================] - 36s 946ms/step - loss: 0.1118 - accuracy: 0.9566 - val_loss: 0.1899 - val_accuracy: 0.9091\n",
            "Epoch 11/50\n",
            "38/38 [==============================] - 37s 983ms/step - loss: 0.0955 - accuracy: 0.9583 - val_loss: 0.3221 - val_accuracy: 0.9091\n",
            "Epoch 12/50\n",
            "38/38 [==============================] - 37s 961ms/step - loss: 0.1109 - accuracy: 0.9649 - val_loss: 0.2030 - val_accuracy: 0.9034\n",
            "Epoch 13/50\n",
            "38/38 [==============================] - 36s 958ms/step - loss: 0.0686 - accuracy: 0.9770 - val_loss: 0.1072 - val_accuracy: 0.9375\n",
            "Epoch 14/50\n",
            "38/38 [==============================] - 38s 992ms/step - loss: 0.1466 - accuracy: 0.9316 - val_loss: 0.6132 - val_accuracy: 0.8523\n",
            "Epoch 15/50\n",
            "38/38 [==============================] - 35s 926ms/step - loss: 0.1115 - accuracy: 0.9533 - val_loss: 0.1903 - val_accuracy: 0.9432\n",
            "Epoch 16/50\n",
            "38/38 [==============================] - 37s 975ms/step - loss: 0.0935 - accuracy: 0.9649 - val_loss: 0.4042 - val_accuracy: 0.8750\n",
            "Epoch 17/50\n",
            "38/38 [==============================] - 37s 970ms/step - loss: 0.0593 - accuracy: 0.9833 - val_loss: 0.2857 - val_accuracy: 0.9148\n",
            "Epoch 18/50\n",
            "38/38 [==============================] - 37s 975ms/step - loss: 0.0260 - accuracy: 0.9917 - val_loss: 0.2382 - val_accuracy: 0.9318\n",
            "Epoch 19/50\n",
            "38/38 [==============================] - 36s 952ms/step - loss: 0.0153 - accuracy: 0.9933 - val_loss: 0.1312 - val_accuracy: 0.9602\n",
            "Epoch 20/50\n",
            "38/38 [==============================] - 38s 1s/step - loss: 0.0317 - accuracy: 0.9833 - val_loss: 0.2309 - val_accuracy: 0.9432\n",
            "Epoch 21/50\n",
            "38/38 [==============================] - 36s 949ms/step - loss: 0.0587 - accuracy: 0.9833 - val_loss: 0.2945 - val_accuracy: 0.9261\n",
            "Epoch 22/50\n",
            "38/38 [==============================] - 36s 940ms/step - loss: 0.0224 - accuracy: 0.9917 - val_loss: 0.1807 - val_accuracy: 0.9375\n",
            "Epoch 23/50\n",
            "38/38 [==============================] - 37s 985ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.4102 - val_accuracy: 0.9148\n",
            "Epoch 24/50\n",
            "38/38 [==============================] - 35s 916ms/step - loss: 0.0042 - accuracy: 0.9983 - val_loss: 0.1934 - val_accuracy: 0.9545\n",
            "Epoch 25/50\n",
            "38/38 [==============================] - 35s 933ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.3035 - val_accuracy: 0.9489\n",
            "Epoch 26/50\n",
            "38/38 [==============================] - 36s 939ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.3588 - val_accuracy: 0.9205\n",
            "Epoch 27/50\n",
            "38/38 [==============================] - 37s 970ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4292 - val_accuracy: 0.9034\n",
            "Epoch 28/50\n",
            "38/38 [==============================] - 37s 961ms/step - loss: 7.0102e-04 - accuracy: 1.0000 - val_loss: 0.3885 - val_accuracy: 0.9261\n",
            "Epoch 29/50\n",
            "38/38 [==============================] - 37s 964ms/step - loss: 5.1459e-04 - accuracy: 1.0000 - val_loss: 0.3164 - val_accuracy: 0.9318\n",
            "Epoch 30/50\n",
            "38/38 [==============================] - 36s 939ms/step - loss: 3.9100e-04 - accuracy: 1.0000 - val_loss: 0.3396 - val_accuracy: 0.9489\n",
            "Epoch 31/50\n",
            "38/38 [==============================] - 37s 962ms/step - loss: 3.8494e-04 - accuracy: 1.0000 - val_loss: 0.3313 - val_accuracy: 0.9489\n",
            "Epoch 32/50\n",
            "38/38 [==============================] - 36s 953ms/step - loss: 2.5862e-04 - accuracy: 1.0000 - val_loss: 0.3903 - val_accuracy: 0.9375\n",
            "Epoch 33/50\n",
            "38/38 [==============================] - 37s 961ms/step - loss: 2.6291e-04 - accuracy: 1.0000 - val_loss: 0.3917 - val_accuracy: 0.9318\n",
            "Epoch 34/50\n",
            "38/38 [==============================] - 37s 964ms/step - loss: 1.8539e-04 - accuracy: 1.0000 - val_loss: 0.3958 - val_accuracy: 0.9375\n",
            "Epoch 35/50\n",
            "38/38 [==============================] - 36s 948ms/step - loss: 2.0629e-04 - accuracy: 1.0000 - val_loss: 0.3742 - val_accuracy: 0.9375\n",
            "Epoch 36/50\n",
            "38/38 [==============================] - 34s 891ms/step - loss: 1.9888e-04 - accuracy: 1.0000 - val_loss: 0.3941 - val_accuracy: 0.9375\n",
            "Epoch 37/50\n",
            "38/38 [==============================] - 38s 993ms/step - loss: 1.5469e-04 - accuracy: 1.0000 - val_loss: 0.4170 - val_accuracy: 0.9261\n",
            "Epoch 38/50\n",
            "38/38 [==============================] - 34s 885ms/step - loss: 1.4090e-04 - accuracy: 1.0000 - val_loss: 0.4189 - val_accuracy: 0.9261\n",
            "Epoch 39/50\n",
            "38/38 [==============================] - 36s 953ms/step - loss: 1.2983e-04 - accuracy: 1.0000 - val_loss: 0.3510 - val_accuracy: 0.9375\n",
            "Epoch 40/50\n",
            "38/38 [==============================] - 37s 987ms/step - loss: 1.2312e-04 - accuracy: 1.0000 - val_loss: 0.4201 - val_accuracy: 0.9261\n",
            "Epoch 41/50\n",
            "38/38 [==============================] - 37s 966ms/step - loss: 9.4102e-05 - accuracy: 1.0000 - val_loss: 0.3787 - val_accuracy: 0.9318\n",
            "Epoch 42/50\n",
            "38/38 [==============================] - 35s 926ms/step - loss: 1.0438e-04 - accuracy: 1.0000 - val_loss: 0.4327 - val_accuracy: 0.9261\n",
            "Epoch 43/50\n",
            "38/38 [==============================] - 36s 956ms/step - loss: 9.5983e-05 - accuracy: 1.0000 - val_loss: 0.4060 - val_accuracy: 0.9432\n",
            "Epoch 44/50\n",
            "38/38 [==============================] - 38s 994ms/step - loss: 8.4862e-05 - accuracy: 1.0000 - val_loss: 0.3740 - val_accuracy: 0.9375\n",
            "Epoch 45/50\n",
            "38/38 [==============================] - 36s 960ms/step - loss: 7.7249e-05 - accuracy: 1.0000 - val_loss: 0.3726 - val_accuracy: 0.9432\n",
            "Epoch 46/50\n",
            "38/38 [==============================] - 36s 949ms/step - loss: 7.0268e-05 - accuracy: 1.0000 - val_loss: 0.4118 - val_accuracy: 0.9375\n",
            "Epoch 47/50\n",
            "38/38 [==============================] - 36s 938ms/step - loss: 6.7769e-05 - accuracy: 1.0000 - val_loss: 0.4044 - val_accuracy: 0.9432\n",
            "Epoch 48/50\n",
            "38/38 [==============================] - 36s 959ms/step - loss: 5.6050e-05 - accuracy: 1.0000 - val_loss: 0.3127 - val_accuracy: 0.9489\n",
            "Epoch 49/50\n",
            "38/38 [==============================] - 35s 915ms/step - loss: 5.3655e-05 - accuracy: 1.0000 - val_loss: 0.3729 - val_accuracy: 0.9489\n",
            "Epoch 50/50\n",
            "38/38 [==============================] - 38s 998ms/step - loss: 5.4117e-05 - accuracy: 1.0000 - val_loss: 0.4174 - val_accuracy: 0.9375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WL1lXJQHQ1wR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Overfit quite a bit, but 93-94% val accuracy on a custom CNN!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCAzWdrf6xSF",
        "colab_type": "text"
      },
      "source": [
        "# Custom CNN Model with Image Manipulations\n",
        "\n",
        "To simulate an increase in a sample of image, you can apply image manipulation techniques: cropping, rotation, stretching, etc. Luckily Keras has some handy functions for us to apply these techniques to our mountain and forest example. Simply, you should be able to modify our image generator for the problem. Check out these resources to help you get started: \n",
        "\n",
        "1. [Keras `ImageGenerator` Class](https://keras.io/preprocessing/image/#imagedatagenerator-class)\n",
        "2. [Building a powerful image classifier with very little data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKt7a1q76xSF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9c253742-2bac-4889-ed63-45724e6b5be9"
      },
      "source": [
        "img_modifier = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    featurewise_center=True,\n",
        "    validation_split=.2,\n",
        "    rescale=1./255\n",
        ")\n",
        "\n",
        "modified_images = img_modifier.flow_from_directory(batch_size=batch_size,\n",
        "                                                           directory=train_dir,\n",
        "                                                           shuffle=True,\n",
        "                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                           class_mode='binary')\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 631 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ui1F2xXSvZJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "72d6ec8c-b641-4793-8eca-977383342e52"
      },
      "source": [
        "val_img_modifier = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    featurewise_center=True,\n",
        "    validation_split=.2,\n",
        "    rescale=1./255\n",
        ")\n",
        "\n",
        "val_modified_images = val_img_modifier.flow_from_directory(batch_size=batch_size,\n",
        "                                                           directory=validation_dir,\n",
        "                                                           shuffle=True,\n",
        "                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                           class_mode='binary')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 195 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhPUt76OTXC6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "7f730fe9-20c0-4247-b01f-4ac8e34ce13b"
      },
      "source": [
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), activation='relu', input_shape=(224, 224, 3)))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "model.add(Flatten()) #flatten before going to fully connected layer (before final)\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 222, 222, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 111, 111, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 109, 109, 64)      18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 54, 54, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 52, 52, 64)        36928     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 173056)            0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 64)                11075648  \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 11,132,033\n",
            "Trainable params: 11,132,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IszntoDxTWiH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qr9FVUIS_16",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c6191d6e-5126-4496-a8c5-14fbc29dbcef"
      },
      "source": [
        "\n",
        "\n",
        "history3 = model.fit(\n",
        "    modified_images,\n",
        "    steps_per_epoch=total_train // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_modified_images,\n",
        "    validation_steps=total_val // batch_size\n",
        ")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "38/38 [==============================] - 36s 959ms/step - loss: 0.7292 - accuracy: 0.7596 - val_loss: 0.4826 - val_accuracy: 0.8068\n",
            "Epoch 2/50\n",
            "38/38 [==============================] - 36s 935ms/step - loss: 0.2940 - accuracy: 0.8932 - val_loss: 0.4865 - val_accuracy: 0.8295\n",
            "Epoch 3/50\n",
            "38/38 [==============================] - 37s 972ms/step - loss: 0.2693 - accuracy: 0.9065 - val_loss: 0.1977 - val_accuracy: 0.9091\n",
            "Epoch 4/50\n",
            "38/38 [==============================] - 36s 954ms/step - loss: 0.2605 - accuracy: 0.8915 - val_loss: 0.1573 - val_accuracy: 0.9375\n",
            "Epoch 5/50\n",
            "38/38 [==============================] - 36s 940ms/step - loss: 0.1984 - accuracy: 0.9243 - val_loss: 0.5580 - val_accuracy: 0.8068\n",
            "Epoch 6/50\n",
            "38/38 [==============================] - 35s 918ms/step - loss: 0.2011 - accuracy: 0.9282 - val_loss: 0.1898 - val_accuracy: 0.9375\n",
            "Epoch 7/50\n",
            "38/38 [==============================] - 37s 975ms/step - loss: 0.1441 - accuracy: 0.9516 - val_loss: 0.2953 - val_accuracy: 0.8977\n",
            "Epoch 8/50\n",
            "38/38 [==============================] - 37s 977ms/step - loss: 0.1037 - accuracy: 0.9649 - val_loss: 0.1618 - val_accuracy: 0.9602\n",
            "Epoch 9/50\n",
            "38/38 [==============================] - 36s 954ms/step - loss: 0.1126 - accuracy: 0.9633 - val_loss: 0.1926 - val_accuracy: 0.9261\n",
            "Epoch 10/50\n",
            "38/38 [==============================] - 34s 898ms/step - loss: 0.0793 - accuracy: 0.9716 - val_loss: 0.1154 - val_accuracy: 0.9489\n",
            "Epoch 11/50\n",
            "38/38 [==============================] - 36s 951ms/step - loss: 0.0591 - accuracy: 0.9833 - val_loss: 0.1609 - val_accuracy: 0.9375\n",
            "Epoch 12/50\n",
            "38/38 [==============================] - 35s 933ms/step - loss: 0.1135 - accuracy: 0.9549 - val_loss: 0.1426 - val_accuracy: 0.9375\n",
            "Epoch 13/50\n",
            "38/38 [==============================] - 35s 928ms/step - loss: 0.1374 - accuracy: 0.9432 - val_loss: 0.1213 - val_accuracy: 0.9489\n",
            "Epoch 14/50\n",
            "38/38 [==============================] - 36s 957ms/step - loss: 0.0647 - accuracy: 0.9733 - val_loss: 0.1349 - val_accuracy: 0.9545\n",
            "Epoch 15/50\n",
            "38/38 [==============================] - 37s 962ms/step - loss: 0.0226 - accuracy: 0.9967 - val_loss: 0.1454 - val_accuracy: 0.9432\n",
            "Epoch 16/50\n",
            "38/38 [==============================] - 38s 988ms/step - loss: 0.0946 - accuracy: 0.9616 - val_loss: 0.2230 - val_accuracy: 0.9432\n",
            "Epoch 17/50\n",
            "38/38 [==============================] - 37s 966ms/step - loss: 0.0458 - accuracy: 0.9816 - val_loss: 0.8568 - val_accuracy: 0.8182\n",
            "Epoch 18/50\n",
            "38/38 [==============================] - 37s 984ms/step - loss: 0.0243 - accuracy: 0.9933 - val_loss: 0.2746 - val_accuracy: 0.9148\n",
            "Epoch 19/50\n",
            "38/38 [==============================] - 35s 928ms/step - loss: 0.2051 - accuracy: 0.9533 - val_loss: 0.3654 - val_accuracy: 0.8864\n",
            "Epoch 20/50\n",
            "38/38 [==============================] - 32s 847ms/step - loss: 0.0796 - accuracy: 0.9733 - val_loss: 0.1527 - val_accuracy: 0.9432\n",
            "Epoch 21/50\n",
            "38/38 [==============================] - 37s 977ms/step - loss: 0.0302 - accuracy: 0.9900 - val_loss: 0.1065 - val_accuracy: 0.9545\n",
            "Epoch 22/50\n",
            "38/38 [==============================] - 35s 932ms/step - loss: 0.0232 - accuracy: 0.9917 - val_loss: 0.2916 - val_accuracy: 0.9205\n",
            "Epoch 23/50\n",
            "38/38 [==============================] - 37s 970ms/step - loss: 0.0823 - accuracy: 0.9766 - val_loss: 0.3389 - val_accuracy: 0.8750\n",
            "Epoch 24/50\n",
            "38/38 [==============================] - 36s 952ms/step - loss: 0.0628 - accuracy: 0.9800 - val_loss: 0.6603 - val_accuracy: 0.8523\n",
            "Epoch 25/50\n",
            "38/38 [==============================] - 37s 963ms/step - loss: 0.0128 - accuracy: 0.9933 - val_loss: 0.1459 - val_accuracy: 0.9659\n",
            "Epoch 26/50\n",
            "38/38 [==============================] - 38s 990ms/step - loss: 0.0203 - accuracy: 0.9933 - val_loss: 0.3074 - val_accuracy: 0.9205\n",
            "Epoch 27/50\n",
            "38/38 [==============================] - 38s 1s/step - loss: 0.0195 - accuracy: 0.9934 - val_loss: 0.4637 - val_accuracy: 0.8977\n",
            "Epoch 28/50\n",
            "38/38 [==============================] - 37s 979ms/step - loss: 0.0120 - accuracy: 0.9933 - val_loss: 0.3391 - val_accuracy: 0.9318\n",
            "Epoch 29/50\n",
            "38/38 [==============================] - 37s 972ms/step - loss: 0.0045 - accuracy: 0.9983 - val_loss: 0.2432 - val_accuracy: 0.9375\n",
            "Epoch 30/50\n",
            "38/38 [==============================] - 36s 959ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4539 - val_accuracy: 0.9034\n",
            "Epoch 31/50\n",
            "38/38 [==============================] - 36s 949ms/step - loss: 6.8816e-04 - accuracy: 1.0000 - val_loss: 0.4601 - val_accuracy: 0.9091\n",
            "Epoch 32/50\n",
            "38/38 [==============================] - 37s 980ms/step - loss: 5.8463e-04 - accuracy: 1.0000 - val_loss: 0.4805 - val_accuracy: 0.9148\n",
            "Epoch 33/50\n",
            "38/38 [==============================] - 36s 946ms/step - loss: 4.5800e-04 - accuracy: 1.0000 - val_loss: 0.4793 - val_accuracy: 0.9148\n",
            "Epoch 34/50\n",
            "38/38 [==============================] - 36s 956ms/step - loss: 3.4473e-04 - accuracy: 1.0000 - val_loss: 0.4648 - val_accuracy: 0.9205\n",
            "Epoch 35/50\n",
            "38/38 [==============================] - 36s 939ms/step - loss: 3.1906e-04 - accuracy: 1.0000 - val_loss: 0.4529 - val_accuracy: 0.9205\n",
            "Epoch 36/50\n",
            "38/38 [==============================] - 35s 933ms/step - loss: 2.6252e-04 - accuracy: 1.0000 - val_loss: 0.4070 - val_accuracy: 0.9205\n",
            "Epoch 37/50\n",
            "38/38 [==============================] - 34s 894ms/step - loss: 2.3866e-04 - accuracy: 1.0000 - val_loss: 0.4752 - val_accuracy: 0.9261\n",
            "Epoch 38/50\n",
            "38/38 [==============================] - 36s 937ms/step - loss: 1.9663e-04 - accuracy: 1.0000 - val_loss: 0.4535 - val_accuracy: 0.9261\n",
            "Epoch 39/50\n",
            "38/38 [==============================] - 37s 984ms/step - loss: 1.7024e-04 - accuracy: 1.0000 - val_loss: 0.5050 - val_accuracy: 0.9205\n",
            "Epoch 40/50\n",
            "38/38 [==============================] - 35s 911ms/step - loss: 1.4111e-04 - accuracy: 1.0000 - val_loss: 0.3775 - val_accuracy: 0.9375\n",
            "Epoch 41/50\n",
            "38/38 [==============================] - 36s 937ms/step - loss: 1.2456e-04 - accuracy: 1.0000 - val_loss: 0.4658 - val_accuracy: 0.9205\n",
            "Epoch 42/50\n",
            "38/38 [==============================] - 35s 933ms/step - loss: 1.0483e-04 - accuracy: 1.0000 - val_loss: 0.4733 - val_accuracy: 0.9261\n",
            "Epoch 43/50\n",
            "38/38 [==============================] - 36s 957ms/step - loss: 9.5491e-05 - accuracy: 1.0000 - val_loss: 0.4275 - val_accuracy: 0.9318\n",
            "Epoch 44/50\n",
            "38/38 [==============================] - 35s 929ms/step - loss: 8.5526e-05 - accuracy: 1.0000 - val_loss: 0.4763 - val_accuracy: 0.9261\n",
            "Epoch 45/50\n",
            "38/38 [==============================] - 38s 993ms/step - loss: 7.6227e-05 - accuracy: 1.0000 - val_loss: 0.5388 - val_accuracy: 0.9148\n",
            "Epoch 46/50\n",
            "38/38 [==============================] - 36s 958ms/step - loss: 4.9839e-05 - accuracy: 1.0000 - val_loss: 0.5288 - val_accuracy: 0.9148\n",
            "Epoch 47/50\n",
            "38/38 [==============================] - 36s 947ms/step - loss: 5.7437e-05 - accuracy: 1.0000 - val_loss: 0.5251 - val_accuracy: 0.9261\n",
            "Epoch 48/50\n",
            "38/38 [==============================] - 36s 957ms/step - loss: 5.8794e-05 - accuracy: 1.0000 - val_loss: 0.5280 - val_accuracy: 0.9205\n",
            "Epoch 49/50\n",
            "38/38 [==============================] - 37s 967ms/step - loss: 5.2236e-05 - accuracy: 1.0000 - val_loss: 0.5026 - val_accuracy: 0.9261\n",
            "Epoch 50/50\n",
            "38/38 [==============================] - 38s 992ms/step - loss: 4.9501e-05 - accuracy: 1.0000 - val_loss: 0.5619 - val_accuracy: 0.9148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlbRMTu7Yfjr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyFciy0lYf3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uT3UV3gap9H6"
      },
      "source": [
        "# Resources and Stretch Goals\n",
        "\n",
        "Stretch goals\n",
        "- Enhance your code to use classes/functions and accept terms to search and classes to look for in recognizing the downloaded images (e.g. download images of parties, recognize all that contain balloons)\n",
        "- Check out [other available pretrained networks](https://tfhub.dev), try some and compare\n",
        "- Image recognition/classification is somewhat solved, but *relationships* between entities and describing an image is not - check out some of the extended resources (e.g. [Visual Genome](https://visualgenome.org/)) on the topic\n",
        "- Transfer learning - using images you source yourself, [retrain a classifier](https://www.tensorflow.org/hub/tutorials/image_retraining) with a new category\n",
        "- (Not CNN related) Use [piexif](https://pypi.org/project/piexif/) to check out the metadata of images passed in to your system - see if they're from a national park! (Note - many images lack GPS metadata, so this won't work in most cases, but still cool)\n",
        "\n",
        "Resources\n",
        "- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) - influential paper (introduced ResNet)\n",
        "- [YOLO: Real-Time Object Detection](https://pjreddie.com/darknet/yolo/) - an influential convolution based object detection system, focused on inference speed (for applications to e.g. self driving vehicles)\n",
        "- [R-CNN, Fast R-CNN, Faster R-CNN, YOLO](https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e) - comparison of object detection systems\n",
        "- [Common Objects in Context](http://cocodataset.org/) - a large-scale object detection, segmentation, and captioning dataset\n",
        "- [Visual Genome](https://visualgenome.org/) - a dataset, a knowledge base, an ongoing effort to connect structured image concepts to language"
      ]
    }
  ]
}